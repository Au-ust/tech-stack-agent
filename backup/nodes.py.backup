"""
LangGraph Node Implementations
"""
import json
from typing import Dict, Any
from rich.console import Console
from rich.prompt import Prompt, Confirm
from rich.panel import Panel

from src.agent.state import TechStackState
from src.utils.llm_client import get_llm_client
from src.tools.search import get_search_tool
from src.utils.file_manager import get_file_manager
from src.prompts.analyzer import (
    ANALYSIS_SYSTEM_PROMPT,
    get_analysis_prompt,
)
from src.prompts.searcher import (
    SEARCH_SYSTEM_PROMPT,
    get_search_keywords_prompt,
)
from src.prompts.generator import (
    GENERATOR_SYSTEM_PROMPT,
    get_generation_prompt,
)

console = Console()


def welcome_node(state: TechStackState) -> Dict[str, Any]:
    """
    Welcome node - greet user and explain the process.
    """
    console.print(Panel.fit(
        "[bold cyan]ğŸš€ å‰ç«¯æŠ€æœ¯æ ˆé€‰å‹ Agent[/bold cyan]\n\n"
        "æˆ‘å°†é€šè¿‡å¼•å¯¼å¼é—®ç­”å¸®åŠ©æ‚¨é€‰æ‹©æœ€åˆé€‚çš„å‰ç«¯æŠ€æœ¯æ ˆã€‚\n\n"
        "æµç¨‹åŒ…æ‹¬ï¼š\n"
        "1. æ”¶é›†é¡¹ç›®ä¿¡æ¯\n"
        "2. åˆ†ææŠ€æœ¯éœ€æ±‚\n"
        "3. åœ¨çº¿è°ƒç ”ï¼ˆå¦‚éœ€è¦ï¼‰\n"
        "4. ç”ŸæˆæŠ€æœ¯é€‰å‹æ–‡æ¡£\n"
        "5. ä¿å­˜æ–‡æ¡£åˆ°æœ¬åœ°\n",
        title="æ¬¢è¿",
        border_style="cyan"
    ))
    
    return {
        "current_step": "welcome",
        "messages": ["ç”¨æˆ·å¼€å§‹ä½¿ç”¨æŠ€æœ¯æ ˆé€‰å‹Agent"]
    }


def ask_project_type_node(state: TechStackState) -> Dict[str, Any]:
    """
    Ask for project type.
    """
    console.print("\n[bold yellow]é—®é¢˜ 1/4[/bold yellow]")
    
    project_type = Prompt.ask(
        "è¯·æè¿°æ‚¨çš„é¡¹ç›®ç±»å‹",
        default="Webåº”ç”¨",
        choices=["Webåº”ç”¨", "ç§»åŠ¨åº”ç”¨", "æ¡Œé¢åº”ç”¨", "å°ç¨‹åº", "æ··åˆåº”ç”¨", "å…¶ä»–"]
    )
    
    return {
        "project_type": project_type,
        "current_step": "ask_type",
        "messages": [f"é¡¹ç›®ç±»å‹: {project_type}"]
    }


def ask_team_size_node(state: TechStackState) -> Dict[str, Any]:
    """
    Ask for team size.
    """
    console.print("\n[bold yellow]é—®é¢˜ 2/4[/bold yellow]")
    
    team_size = Prompt.ask(
        "è¯·é€‰æ‹©æ‚¨çš„å›¢é˜Ÿè§„æ¨¡",
        choices=["1-3äººï¼ˆå°å‹å›¢é˜Ÿï¼‰", "4-10äººï¼ˆä¸­å‹å›¢é˜Ÿï¼‰", "10äººä»¥ä¸Šï¼ˆå¤§å‹å›¢é˜Ÿï¼‰"],
        default="1-3äººï¼ˆå°å‹å›¢é˜Ÿï¼‰"
    )
    
    return {
        "team_size": team_size,
        "current_step": "ask_team",
        "messages": [f"å›¢é˜Ÿè§„æ¨¡: {team_size}"]
    }


def ask_timeline_node(state: TechStackState) -> Dict[str, Any]:
    """
    Ask for development timeline.
    """
    console.print("\n[bold yellow]é—®é¢˜ 3/4[/bold yellow]")
    
    timeline = Prompt.ask(
        "è¯·é€‰æ‹©é¢„æœŸçš„å¼€å‘æ—¶é—´çº¿",
        choices=["1ä¸ªæœˆå†…", "1-3ä¸ªæœˆ", "3-6ä¸ªæœˆ", "6ä¸ªæœˆä»¥ä¸Š"],
        default="1-3ä¸ªæœˆ"
    )
    
    return {
        "timeline": timeline,
        "current_step": "ask_timeline",
        "messages": [f"å¼€å‘æ—¶é—´çº¿: {timeline}"]
    }


def ask_special_requirements_node(state: TechStackState) -> Dict[str, Any]:
    """
    Ask for special requirements.
    """
    console.print("\n[bold yellow]é—®é¢˜ 4/4[/bold yellow]")
    
    special_requirements = Prompt.ask(
        "è¯·æè¿°ä»»ä½•ç‰¹æ®Šéœ€æ±‚ï¼ˆå¦‚SEOã€é«˜æ€§èƒ½ã€å®æ—¶é€šä¿¡ç­‰ï¼ŒæŒ‰å›è½¦è·³è¿‡ï¼‰",
        default="æ— ç‰¹æ®Šéœ€æ±‚"
    )
    
    return {
        "special_requirements": special_requirements,
        "current_step": "ask_special",
        "messages": [f"ç‰¹æ®Šéœ€æ±‚: {special_requirements}"]
    }


def analyze_node(state: TechStackState) -> Dict[str, Any]:
    """
    Analyze user requirements using LLM.
    """
    console.print("\n[bold green]ğŸ” æ­£åœ¨åˆ†æé¡¹ç›®éœ€æ±‚...[/bold green]")
    
    # Prepare project info
    project_info = {
        'project_type': state.get('project_type', ''),
        'team_size': state.get('team_size', ''),
        'timeline': state.get('timeline', ''),
        'special_requirements': state.get('special_requirements', ''),
    }
    
    # Get LLM client
    llm_client = get_llm_client()
    
    # Generate analysis prompt
    prompt = get_analysis_prompt(project_info)
    
    try:
        # Call LLM
        response = llm_client.invoke(prompt, system_message=ANALYSIS_SYSTEM_PROMPT)
        
        # Parse JSON response
        # Extract JSON from markdown code blocks if present
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            json_str = response[json_start:json_end].strip()
        elif "```" in response:
            json_start = response.find("```") + 3
            json_end = response.find("```", json_start)
            json_str = response[json_start:json_end].strip()
        else:
            json_str = response.strip()
        
        analysis_result = json.loads(json_str)
        
        # Display results
        console.print("\n[bold cyan]åˆ†æç»“æœï¼š[/bold cyan]")
        console.print(f"âœ“ æå–äº† {len(analysis_result.get('extracted_requirements', []))} ä¸ªæ ¸å¿ƒéœ€æ±‚")
        console.print(f"âœ“ è¯†åˆ«äº† {len(analysis_result.get('tech_constraints', []))} ä¸ªæŠ€æœ¯çº¦æŸ")
        console.print(f"âœ“ æ˜¯å¦éœ€è¦åœ¨çº¿æœç´¢: {'æ˜¯' if analysis_result.get('needs_search', False) else 'å¦'}")
        
        return {
            "extracted_requirements": analysis_result.get('extracted_requirements', []),
            "tech_constraints": analysis_result.get('tech_constraints', []),
            "needs_search": analysis_result.get('needs_search', False),
            "current_step": "analyze",
            "messages": [f"éœ€æ±‚åˆ†æå®Œæˆ: {len(analysis_result.get('extracted_requirements', []))} ä¸ªéœ€æ±‚"]
        }
    
    except Exception as e:
        console.print(f"[red]åˆ†æå¤±è´¥: {str(e)}[/red]")
        # Fallback: no search needed
        return {
            "extracted_requirements": ["åŸºäºé¡¹ç›®ç±»å‹çš„æ ‡å‡†éœ€æ±‚"],
            "tech_constraints": ["å›¢é˜Ÿå­¦ä¹ æ›²çº¿"],
            "needs_search": False,
            "current_step": "analyze",
            "messages": ["åˆ†æé‡åˆ°é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"]
        }


def search_node(state: TechStackState) -> Dict[str, Any]:
    """
    Perform online search for technology information.
    """
    console.print("\n[bold green]ğŸŒ æ­£åœ¨è¿›è¡ŒæŠ€æœ¯è°ƒç ”...[/bold green]")
    
    # Prepare project info and analysis result
    project_info = {
        'project_type': state.get('project_type', ''),
        'team_size': state.get('team_size', ''),
        'timeline': state.get('timeline', ''),
        'special_requirements': state.get('special_requirements', ''),
    }
    
    analysis_result = {
        'extracted_requirements': state.get('extracted_requirements', []),
        'tech_constraints': state.get('tech_constraints', []),
    }
    
    # Get LLM client and search tool
    llm_client = get_llm_client()
    search_tool = get_search_tool()
    
    try:
        # Generate search keywords using LLM
        prompt = get_search_keywords_prompt(project_info, analysis_result)
        response = llm_client.invoke(prompt, system_message=SEARCH_SYSTEM_PROMPT)
        
        # Parse JSON response
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            json_str = response[json_start:json_end].strip()
        elif "```" in response:
            json_start = response.find("```") + 3
            json_end = response.find("```", json_start)
            json_str = response[json_start:json_end].strip()
        else:
            json_str = response.strip()
        
        search_data = json.loads(json_str)
        keywords = search_data.get('search_keywords', [])
        
        console.print(f"ç”Ÿæˆäº† {len(keywords)} ä¸ªæœç´¢å…³é”®è¯")
        
        # Perform searches (limit to first 5 keywords to save time)
        all_results = []
        for keyword in keywords[:5]:
            console.print(f"  æœç´¢: {keyword}")
            results = search_tool.search(keyword, max_results=3)
            all_results.extend(results)
        
        console.print(f"âœ“ æ‰¾åˆ° {len(all_results)} æ¡ç›¸å…³ä¿¡æ¯")
        
        return {
            "search_results": all_results,
            "current_step": "search",
            "messages": [f"å®ŒæˆæŠ€æœ¯è°ƒç ”ï¼Œæ”¶é›†äº† {len(all_results)} æ¡ä¿¡æ¯"]
        }
    
    except Exception as e:
        console.print(f"[yellow]æœç´¢é‡åˆ°é—®é¢˜: {str(e)}ï¼Œå°†ç»§ç»­ä½¿ç”¨å·²æœ‰çŸ¥è¯†ç”Ÿæˆæ–‡æ¡£[/yellow]")
        return {
            "search_results": [],
            "current_step": "search",
            "messages": ["æœç´¢å¤±è´¥ï¼Œä½¿ç”¨LLMå·²æœ‰çŸ¥è¯†"]
        }


def generate_node(state: TechStackState) -> Dict[str, Any]:
    """
    Generate the complete technical document.
    """
    console.print("\n[bold green]ğŸ“ æ­£åœ¨ç”ŸæˆæŠ€æœ¯é€‰å‹æ–‡æ¡£...[/bold green]")
    
    # Prepare all input data
    project_info = {
        'project_type': state.get('project_type', ''),
        'team_size': state.get('team_size', ''),
        'timeline': state.get('timeline', ''),
        'special_requirements': state.get('special_requirements', ''),
    }
    
    analysis_result = {
        'extracted_requirements': state.get('extracted_requirements', []),
        'tech_constraints': state.get('tech_constraints', []),
    }
    
    search_results = state.get('search_results', [])
    
    # Get LLM client
    llm_client = get_llm_client()
    
    try:
        # Generate document prompt
        prompt = get_generation_prompt(project_info, analysis_result, search_results)
        
        # Call LLM with streaming for better UX
        console.print("\n[dim]ç”Ÿæˆä¸­...[/dim]")
        document_parts = []
        
        for chunk in llm_client.stream(prompt, system_message=GENERATOR_SYSTEM_PROMPT):
            document_parts.append(chunk)
        
        final_document = "".join(document_parts)
        
        console.print("âœ“ æ–‡æ¡£ç”Ÿæˆå®Œæˆ")
        
        return {
            "final_document": final_document,
            "current_step": "generate",
            "messages": ["æŠ€æœ¯æ–‡æ¡£ç”Ÿæˆå®Œæˆ"]
        }
    
    except Exception as e:
        console.print(f"[red]æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}[/red]")
        # Generate a minimal fallback document
        fallback_doc = f"""# æŠ€æœ¯æ ˆé€‰å‹æ–‡æ¡£

## é¡¹ç›®ä¿¡æ¯
- é¡¹ç›®ç±»å‹: {project_info.get('project_type', 'æœªçŸ¥')}
- å›¢é˜Ÿè§„æ¨¡: {project_info.get('team_size', 'æœªçŸ¥')}
- æ—¶é—´çº¿: {project_info.get('timeline', 'æœªçŸ¥')}

## æ¨èæŠ€æœ¯æ ˆ
ï¼ˆæ–‡æ¡£ç”Ÿæˆé‡åˆ°é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIé…ç½®åé‡è¯•ï¼‰
"""
        return {
            "final_document": fallback_doc,
            "current_step": "generate",
            "messages": ["æ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œç”Ÿæˆäº†ç®€åŒ–ç‰ˆæœ¬"]
        }


def save_node(state: TechStackState) -> Dict[str, Any]:
    """
    Save the generated document to local file.
    """
    console.print("\n[bold green]ğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡æ¡£...[/bold green]")
    
    final_document = state.get('final_document', '')
    project_type = state.get('project_type', 'unknown')
    
    # Get file manager
    file_manager = get_file_manager()
    
    try:
        # Save document
        output_path = file_manager.save_document(
            content=final_document,
            project_name=project_type
        )
        
        console.print(f"âœ“ æ–‡æ¡£å·²ä¿å­˜åˆ°: [cyan]{output_path}[/cyan]")
        
        # Show preview option
        if Confirm.ask("\næ˜¯å¦æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆï¼Ÿ", default=False):
            console.print("\n" + "="*80)
            console.print(final_document[:500] + "...\nï¼ˆä»…æ˜¾ç¤ºå‰500å­—ç¬¦ï¼‰")
            console.print("="*80)
        
        return {
            "output_path": output_path,
            "current_step": "save",
            "messages": [f"æ–‡æ¡£å·²ä¿å­˜: {output_path}"]
        }
    
    except Exception as e:
        console.print(f"[red]ä¿å­˜å¤±è´¥: {str(e)}[/red]")
        return {
            "output_path": "",
            "current_step": "save",
            "messages": ["ä¿å­˜å¤±è´¥"]
        }
